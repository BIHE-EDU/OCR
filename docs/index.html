<!DOCTYPE html>
<html lang="en" class="wide wow-animation">
<head>
    <!--Project Title-->
    <meta charset="UTF-8">
    <!--*********************** EDIT BELOW **********************************-->
    <title>OCR</title>
    <!--*********************** EDIT ABOVE **********************************-->
    <meta name="format-detection" content="telephone=no">
    <meta name="viewport"
          content="width=device-width, height=device-height, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">


    <!--Stylesheets-->

    <link rel="icon" href="img/favicon.png" type="image/x-icon">

    <!--Bootstrap-->

    <link rel="stylesheet" href="css/style.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML'
            async></script>


    <!--[if lt IE 10]>
        <div style="background: #212121; padding: 10px 0; box-shadow: 3px 3px 5px 0 rgba(0,0,0,.3); clear: both; text-align:center; position: relative; z-index:1;"><a href="http://windows.microsoft.com/en-US/internet-explorer/.."><img src="images/ie8-panel/warning_bar_0000_us.jpg" border="0" height="42" width="820" alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today."></a></div>
        <script src="js/html5shiv.min.js"></script><[endif]-->
</head>


<body>

<div class="page">
    <!--
    ========================================================
                            HEADER
    ========================================================
    -->
    <header class="page-header">

        <!--RD Navbar-->

        <div class="rd-navbar-wrap">
            <nav class="rd-navbar top-panel-none-items" data-layout="rd-navbar-fixed" data-hover-on="false"
                 data-stick-up="false" data-sm-layout="rd-navbar-fullwidth" data-md-layout="rd-navbar-static">

                <div class="rd-navbar-inner">

                    <!--RD Navbar Panel-->

                    <div class="rd-navbar-panel">

                        <!--RD Navbar Toggle-->

                        <button data-rd-navbar-toggle=".rd-navbar" class="rd-navbar-toggle"><span></span></button>

                        <!--END RD Navbar Toggle-->

                        <!--RD Navbar Brand-->

                        <!--END RD Navbar Brand-->

                    </div>

                    <!--END RD Navbar Panel-->

                    <!--RD Navbar Search-->

                    <!--END RD Navbar Search-->

                    <!--RD Navbar Nav-->

                    <ul class="rd-navbar-nav">

                        <li class="active"><a href="index.html">Home</a></li>

                        <li><a href="#">Table of Contents</a>
                            <ul class="rd-navbar-dropdown">

                                <!--*********************** EDIT BELOW **********************************-->
                                <li><a href="#Abstract">Abstract</a></li>
                                <li><a href="#1. Introduction">1. Introductions</a></li>
                                <li><a href="#2. Some Background on Optical Character Recognition">2. Some Background on
                                    Optical Character Recognitionn</a></li>
                                <li class="dropdown men active "><a
                                        href="#3. About the Persian Writings and the Target Font Used"
                                        class="dropdown-toggle" data-toggle="dropdown">3. About the Persian Writings and
                                    the Target Font Used<b class="caret"></b></a>
                                    <ul class="dropdown-menu">
                                        <li><a href="#3.1. Properties of Persian Scripts">3.1. Properties of Persian
                                            Scripts</a>
                                        </li>
                                        <li><a href="#3.2. Properties of the Target Font and Text">3.2. Properties of
                                            the Target
                                            Font and Text</a></li>
                                        <li><a href="#3.3. Selection of Character Classes Covered by OCR System">3.3.
                                            Selection
                                            of Character Classes Covered by OCR System</a></li>
                                    </ul>
                                </li>
                                <li><a href="#4. The Main Recognition Approach">4. The Main Recognition Approach</a>
                                </li>
                                <li class="dropdown men active "><a href="#5. Preprocessing on the Scanned Documents"
                                                                    class="dropdown-toggle" data-toggle="dropdown">5.
                                    Preprocessing on the Scanned Documents<b class="caret"></b></a>
                                    <ul class="dropdown-menu">
                                        <li><a href="#5.1. Noise Removal">5.1. Noise Removal</a></li>
                                        <li><a href="#5.2. Skew Detection">5.2. Skew Detection</a></li>
                                        <li><a href="#5.3. Baseline Detection">5.3. Baseline Detection</a></li>
                                    </ul>
                                </li>
                                <li><a href="#6. Features Extraction">6. Features Extraction</a></li>
                                <li class="dropdown men active "><a href="#7. Training Process" class="dropdown-toggle"
                                                                    data-toggle="dropdown">7. Training Process<b
                                        class="caret"></b></a>
                                    <ul class="dropdown-menu">
                                        <li><a href="#7.1. Preparing the Training Character Images">7.1. Preparing the
                                            Training Character Images</a></li>
                                        <li><a href="#7.2. Computing Fundamental Parameters of Emission Probabilities">7.2.
                                            Computing Fundamental Parameters of Emission Probabilities</a></li>
                                        <li><a href="#7.3. Computing Language Model Probabilities">7.3. Computing
                                            Language Model Probabilities</a></li>
                                        <li class="dropdown men active "><a
                                                href="#7.3. Computing Language Model Probabilities"
                                                class="dropdown-toggle" data-toggle="dropdown">7.3. Computing Language
                                            Model Probabilities<b class="caret"></b></a>
                                            <ul class="dropdown-menu">
                                                <li><a href="#7.3.1. Transition Probabilities">7.3.1. Transition
                                                    Probabilities</a></li>
                                                <li><a href="#7.3.1. Transition Probabilities">7.3.1. Transition
                                                    Probabilities</a></li>
                                                <li><a href="#7.3.1. Transition Probabilities">7.3.1. Transition
                                                    Probabilities</a></li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                                <li><a href="bibliography.html">Bibliography</a></li>
                                <li><a href="paper.html">Thesis in PDF</a></li>

                                <!--*********************** EDIT ABOVE **********************************-->
                            </ul>
                        </li>
                    </ul>

                    <!--END RD Navbar Nav-->

                </div>
            </nav>
        </div>
        <!--END RD Navbar-->

        <!--Swiper-->
        <section>

            <div class="rd-parallax">
                <div data-speed="0.6" data-type="media" data-url="img/boats.jpg"
                     class="rd-parallax-layer"></div>
                <div data-speed="0.78" data-type="html" data-fade="true"
                     class="well-parallax jumbotron text-center rd-parallax-layer">
                    <!--*********************** EDIT BELOW **********************************-->
                    <h1 class="text-white">Applying Optical Character Recognition (OCR) for Recognizing Texts Written in
                        a Specific Persian Font</h1>
                    <h4 class="text-white">A thesis submitted to BIHE for the degree of B.S. in the faculty of Computer
                        Engineering</h4>
                    <h5 class="text-white">Submitted by: Vargha Dadvar</h5>
                    <h5 class="text-white">Submission Date: July 2013 </h5>
                    <h5 class="text-white">Project Advisor: Fares Hedayati</h5>
                    <!--*********************** EDIT ABOVE **********************************-->
                </div>
            </div>
        </section>
        <!--END Swiper-->

    </header>
</div>

<!--*********************** EDIT BELOW **********************************-->
<!--Start section-->
<a name="Abstract"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>Abstract</h3>
        <p class="lead para">Recognition of the text in scanned documents of older Persian writings is a challenging
            task, because of the
            properties of such scripts and the cursive property of Persian writings in general. As separate segmentation
            of Persian characters is much more difficult than in Latin and some other alphabets, using segmentation-free
            approaches are more useful in Persian OCR. In this project, one of such methods called the Generalized
            Hidden Markov Models (or gHMM) is used for the recognition of texts written in a specific Persian font,
            which was mostly used in older Persian documents and books. The gHMM not only helps to combine text line
            segmentation and character recognition processes, but also provides a way to take advantage of both the
            character image properties and the language model in the service of character recognition. The language
            model probabilities and the parameters computed from the training character features are used in the
            recognition process, and should be prepared during the training phase. The main recognition program
            implemented in this project firstly detects and corrects the skew of the input pages and detects the text
            lines. Then in the body of the recognition process, a generalized version of the Viterbi algorithm is used
            to recognize simultaneously the best segmentation and the best character sequence corresponding to each text
            line. The Modified Quadratic Distance Function (or MQDF) is used in the heart of the Viterbi algorithm in
            order to measure the closeness of each character image with each of the character classes. The recognition
            accuracy of the implemented OCR program is 85.5%.</p>
    </div>
</section>
<!--End section-->
<!--Start section-->
<a name="1. Introduction"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>1. Introduction</h3>
        <p class="lead para">This paper is the final document of my final thesis for the B.S. degree of Computer
            Engineering in BIHE. This
            project includes a study about different aspects of the Optical Character Recognition problem, and also the
            implementation of an OCR system for a specific Persian font used in the printed version of some older
            Persian books. In this document, the theoretical foundations and the practical considerations of designing
            and implementing the OCR program are explained.</p>
        <p class="lead para"> Optical Character Recognition (OCR) is the process of recognizing the text in the scanned
            images of the
            documents or books. Despite advances in the recognition of Latin writings and texts written in some other
            alphabets, research in Persian and Arabic OCR and handwriting recognition still continues. Some background
            information about the OCR problem is given in Section 2. As one of the possible applications of Persian OCR,
            there are many historical and older Persian books that should be converted to computer-readable text. In
            this project, one of the fonts and writing styles used in such older Persian books is chosen, and an OCR
            program is implemented for recognizing the texts written in this font. Persian writings are cursive and most
            of them include some other complexities, as will be explained in Section 3, and this makes the recognition
            of such texts difficult and more complex.</p>
        <p class="lead para">Hidden Markov Models (HMM) is the main mathematical model used in this project for solving
            the character
            recognition problem, as it is detailed in Section 4. HMM helps to use both the target language properties
            and image properties of the characters for the recognition process. A generalized version of HMM also helps
            to combine the text line segmentation and the character recognition processes, as this approach is
            preferable in Persian OCR.
            Before the text line images are used as inputs to the recognition process, some preprocessing should be
            performed on the scanned page images, such as skew detection and baseline detection, which will be explained
            in Section 5. Section 6 introduces the features which are used in this project to capture the properties of
            character images, so that the appearance of the characters can be distinguishable by the OCR program. As the
            OCR system should be trained to have the capability of recognizing the characters of a text line image, a
            separate training process should be done. In this phase, the language model probabilities and the parameters
            needed for predicting the character corresponding to each character image are trained (i.e. prepared) by
            analyzing the type-written text version and the scanned images of a training text document. The details are
            explained in Section 7.</p>
        <p class="lead para">In Section 8, the concept of distance functions is introduced as a special function which
            is used to compute
            the level of closeness between each character image and a character class. This function is used in the body
            the Viterbi algorithm, which is responsible to find the most probable character sequence for an input text
            line image. Also in this section, we will see how the generalized Viterbi algorithm is used in this project
            to find the best segmentation of the line and the best character sequence simultaneously. Section 9 includes
            a brief explanation about the software technologies used to implement this OCR system. The results of
            testing the implemented OCR program are discussed in Section 10, in addition to the explanation of some
            tasks done in order to improve the accuracy of the system.</p>
    </div>
</section>
<!--End section-->
<!--Start section-->
<a name="2. Some Background on Optical Character Recognition"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>2. Some Background on Optical Character Recognition</h3>
        <p class="lead para">Optical Character Recognition (OCR) is an Artificial Intelligence field which aims to
            correctly detect and recognize the text sequences in scanned documents and writings. Recognizing the text in
            such documents makes these sources of information searchable and manipulable by computers, and prepares them
            to be used as input for text processing and many other computer applications. OCR can be used in different
            types of applications, including the recognition of handwritten documents, recognition of machine-printed
            documents, online recognition of text input in PDAs and smart-phones, recognition of numbers in forms and
            car number plates, etc.</p>
        <p class="lead para"> OCR technology and software have been developed quickly in recent years, and many
            developed OCR solutions
            offer very accurate recognition of text with very low error rates. This level of accuracy in OCR technology
            has become possible in part because of the advancements in Machine Learning, an area in Artificial
            Intelligence which helps to promote methods for learning knowledge and logic from large amounts of data.
            Despite these developments in the recognition of texts written in Latin and some other alphabets, OCR
            research still continues in certain more specific areas, including recognition of texts in other languages
            such as Arabic and Persian scripts, as well as handwriting recognition in different alphabets.
            In almost all of the OCR methods and implementations, certain types of tasks should be done sequentially in
            the recognition process, including noise removal, document skew detection, baseline detection, character
            segmentation, and finally character or text recognition. In some languages, segmenting the text line into
            characters is easier because of the detectable space between the characters. However, Arabic and Persian
            texts are cursive and therefore automatic character segmentation is a very difficult task in such languages.
            As a result, most of the methods proposed
            for Arabic and Persian OCR use a holistic approach, meaning that the characters are not separated from each
            other before the recognition process, and the recognition is done in word or text block level. A survey and
            review of the most important researches in Arabic and Persian OCR and handwritten recognition can be found
            in [1] and [2].</p>
        <p class="lead para"> Among the methods with holistic approach to character segmentation, those using Hidden
            Markov Models are of
            great importance. Hidden Markov Models (HMM) make it possible to take advantage of both the language model
            and the image properties of characters in order to recognize the text sequences correctly. One approach in
            using HMM for OCR is to use a universal model that is composed of smaller interconnected character models,
            each of which represents one letter from the alphabet [3]. In another method which is useful for small
            vocabularies of words, a separate HMM is used for each word class [4]. Another way to use HMM without a
            previous character segmentation phase is to combine the recognition process and the segmentation process.
            That is, the final recognition process iterates over all possible segmentation states, and for each of them
            computes the best character sequence. In [5] this method is called Generalized Hidden Markov Models (gHMM),
            and is successfully used for recognition of Tibetan wood block prints.</p>
        <p class="lead para">In our project, the aforementioned Generalized Hidden Markov Models is used as the main
            recognition
            methodology in order to recognize the text in some older Persian writings. Because of the learning purposes
            of this project, the scope of the target texts and fonts chosen are limited, and we concentrated on a
            specific older Persian font for this project. This helps to concentrate on implementing different stages of
            the OCR process in this period, rather than spending time on gathering training data for different types of
            Persian fonts and writing styles, and addressing the wide range of requirements of these different fonts and
            texts.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="3. About the Persian Writings and the Target Font Used"></a>
<a name="3.1. Properties of Persian Scripts"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>3. About the Persian Writings and the Target Font Used</h3>
    </div>
    <div class="container">
        <h3>3.1. Properties of Persian Scripts</h3>
        <p class="lead para">Persian alphabet includes 32 main letters, four more than the main characters of Arabic.
            Most properties of Persian and Arabic alphabet are the same. As in Latin alphabet where each character has
            two different forms (capital and small), in Persian and Arabic each letter has four different forms based on
            the place of a character in the word and the characters before and after it. These forms of a character
            include initial form, medial form, final form, and the isolated form. In Table 1, different forms of some
            Persian letters are shown. It can be seen that the difference
            between some characters are only in the number of dots or other extra strokes they have, and their main body
            is almost the same. For example the letters KAAF (ک) and GAAF (گ) only differ in the extra stroke above
            GAAF, and the difference between characters TEH (ت) and SEH (ث) is in the number of dots above their main
            body. It would be very challenging for the OCR system to distinguish between such similar characters.</p>
        <figure class="figure">
            <img src="img/Persian%20Alphabet.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%" alt="Different forms of some of the characters of the Persian alphabet.">
            <figcaption class="figure-caption text-center">Table 1 - Different forms of some of the characters of the
                Persian alphabet.
            </figcaption>
        </figure>
        <p>As different forms of characters suggest, Persian writings are cursive, and most of the characters are
            connected
            to each other within a word. This characteristic imposes a great challenge to text segmentation in Persian
            writings. The lack of space between characters is sometimes even more intense so that the two adjacent
            characters have vertical overlap with each other. Moreover, some characters are deformed when connected to
            each
            other which form a ligature, as another complexity in Persian and Arabic scripts. Also in Arabic and
            Persian,
            sometimes a character is accompanied with diacritics or symbols above or below its body, mainly the vowel
            symbols. These diacritics are different from dots or other symbols which are inherent to some characters.
            Some
            of these complexities in Persian and Arabic scripts are illustrated in Figure 1 below.</p>
        <figure class="figure">
            <img src="img/Persian Complex Forms.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="Some of the complexities in the form of characters in Persian and Arabic writings">
            <figcaption class="figure-caption text-center">Figure 1 - Some of the complexities in the form of characters
                in Persian and Arabic writings
            </figcaption>
        </figure>

    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="3.2. Properties of the Target Font and Text"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>3.2. Properties of the Target Font and Text</h3>
        <p class="lead para">The font analyzed in this project is a rather old typewritten font used in some older
            Persian writings. As an example, this writing style was used in the printed version of a series of holy
            Baha’i books called ‘Maede-ye-Asemani’ (مائده ی آسمانی ). Some volumes of this book were used as the main
            training and test writings of this project. In addition to the complexities of Persian and Arabic writings
            mentioned above, the target Persian font used in this project also has its own properties. Firstly, there is
            little regularity in the way spaces are used within and between words in the target text. For example, in
            some situations where words are finished with characters like REH (ر) or VAAV (و), there is no space between
            the current word and the next word, and sometimes there is overlap between the final character of a word and
            the first character of the next word. On the other hand, in some situations an extra space is seen between
            the characters of a single word. Figure 2 shows examples of such irregularities in the used document.</p>
        <figure class="figure">
            <img src="img/Irregularities.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="Examples of irregularities in the way spaces are used in the text used in this project.">
            <figcaption class="figure-caption text-center">Figure 2 - Examples of irregularities in the way spaces are
                used in the text used in this project.
            </figcaption>
        </figure>
        <p>In many typewritten Persian texts (mostly the older ones), sometimes the characters are stretched for text
            justification purposes, and this leads to some deformation in the appearance of the characters. Also in the
            target texts of this project, sometimes at the end of lines one or two characters are translated a little
            above the baseline. These complexities are shown in figures below. All these irregularities impose
            challenges to the recognition process.</p>
        <figure class="figure">
            <img src="img/Stretch.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="Examples of character stretch and character translation in the target text.">
            <figcaption class="figure-caption text-center">Figure 3 - Examples of character stretch and character
                translation in the target text.
            </figcaption>
        </figure>

    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="3.3. Selection of Character Classes Covered by OCR System"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>3.3. Selection of Character Classes Covered by OCR System</h3>
        <p class="lead para">Selection of the character classes to be covered by the OCR system was a difficult task,
            mainly because of the irregularities regarding the potential vertical overlaps between adjacent characters.
            Firstly, although some forms of the same characters have very similar appearance, we need to regard these
            similar forms as separate character classes in the OCR system, mainly because the transition behavior of
            various forms of a single character are different (Refer to Section 4 about the definition of transition
            model). For example, final form of the letter NOON (ن) always joins a middle or initial form of a character,
            while the isolated form of NOON (always follows an isolated or final character in a word. In addition to
            different forms of the main characters, some other characters and symbols were also chosen to be included in
            the list of character classes, mainly based on their frequency in the target texts. Arabic digits from 0 to
            9, some symbols like colon and parentheses, and some other Arabic characters like HAMZEH (ء) and
            TEH_MARBUTEH (ة) are among such characters.</p>
        <p class="lead para">As mentioned above, characters which have vertical overlaps when connected to each other
            can be sources of
            error in the recognition process. The reason is that during the recognition process, in such cases finding
            the best segmentation point is very difficult for the algorithm, and thus overlapping characters may not be
            segmented and thus recognized correctly. Therefore, it may be a good practice to also include some
            combinations of characters in the list of character classes, especially those combinations representing
            overlapping characters. This decision can help the software to recognize combinations of characters where
            the single characters are tightly connected and are vertically overlapping each other. In addition, some
            characters like ALEPH (ا) are
            so small in width that it may be better to consider their combinations with some other characters as new
            character classes. However, using combinatorial character classes has its own problems and limitations.
            Firstly, hundreds or thousands of such combinations are present in Persian scripts like the writings used in
            this project, and it is not feasible to include all these combinations as separate character classes.
            Moreover, the overlapping characters problem is not just limited to two-character combinations. For example,
            a sub-word like “ ورا ” in our target texts involves vertical overlaps between VAAV (و) and REH (ر) and
            between REH (and ALEPH (ا). Because of these considerations, we decided to only include some more frequent
            combinatorial character classes in which the characters were tightly connected or were overlapping each
            other a lot.</p>
        <p class="lead para"> On the whole, about 320 number of character classes were selected to be covered by the
            implemented OCR
            software in initial tests. In order to increase the accuracy and stability of the program, some of the
            character classes without a least number of training examples were eliminated in the final tests of the
            project.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="4. The Main Recognition Approach"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>4. The Main Recognition Approach</h3>
        <p class="lead para">In this section, the main recognition methodology used in the project is explained, as well
            as
            the related theoretical foundations. As mentioned previously in this document, Hidden Markov Model (HMM) is
            the
            main mathematical tool used to solve the OCR problem in this project. In a probabilistic problem modeled by
            HMM,
            there are some random variables which are sequentially dependent on each other, as Figure 4 shows. This
            means
            that each of these variables is only dependent on its previous adjacent variable. The values of these random
            variables are unknown and therefore these variables are called hidden state variables. Hidden variables can
            have
            one of the possible discrete values or states , , …, . The probability is the probability that the hidden
            variable has value and its previous adjacent variable has value . Such a probability is called the
            Transition
            Probability. Also there is another class of random variables , each of which corresponding to one hidden
            variable, whose values are known and can be either discrete or continuous. They are called observation
            variables
            (shown in the lower part of Figure 4). The probability is the probability that the observation corresponds
            to
            the value of the hidden state . In the terminology of HMMs, this type of probability is called the Emission
            Probability.</p>
        <figure class="figure">
            <img src="img/Hidden markov.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="A diagram showing the dependency relations between variables in a Hidden Markov Model.">
            <figcaption class="figure-caption text-center">Figure 4 - A diagram showing the dependency relations between
                variables in a Hidden Markov Model.
            </figcaption>
        </figure>
        <p class="lead para">According to the probability theory, if the values of all observation variables and all
            possible transition and emission probabilities are known, then the joint probability of occurrence of each
            possible sequence of values for hidden state variables can be computed as follows:</p>
        <figure class="figure">
            <img src="img/Formula1.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="70%" height="70%"
                 alt="Formula">
        </figure>
        <p class="lead para"> It is obvious that only the first hidden variable does not have any adjacent variable to
            its
            left. Therefore, instead of a transition probability from its previous variable, its own probability is
            multiplied in the formula. This probability is called the Initial Probability and is only computed for the
            initial hidden variable.</p>
        <p class="lead para">Now the same method can be used to model the character recognition problem. If a text line
            image is segmented
            into number of character images, then the observations are these character images. The actual character
            classes
            corresponding to each of these observations are unknown, and thus the guesses about the identities of these
            observations are the hidden variables of the model. Transition probability is the probability of the
            adjacency
            of two characters. Therefore, if we have number of character classes, an x matrix can hold all the
            transition
            probabilities between characters. The details of computing language model probabilities are explained in
            Section
            7. Emission probability is the probability that an observation (i.e. a segment of the line image)
            corresponds to
            a specific character class. It is a measure for the level of similarity between the image and the character
            class. As we will see in Section 8, a function similar to a Gaussian distribution is used to compute the
            emission probabilities for different segments of a line and for different character classes. The initial
            probabilities are also computed by counting the number of occurrences of each character in the language
            model.
            When all these probabilities are available, the probability that a set of character images corresponds to a
            set
            of character classes can be computed using the formula
            above for all possible sequences of characters. Therefore, it is possible to find the best sequence of
            characters which maximizes the probability <img src="img/Probability.png"
                                                            class="figure-img img-fluid rounded center-block"
                                                            data-html="true"
                                                            width="25%" height="25%"
                                                            alt="Probability"> This process of finding the best
            character sequence for a line is
            performed in an algorithm called Viterbi. In this way, the Hidden Markov Models enable us to combine
            computer
            vision techniques (emission probabilities) and language model (transition and initial probabilities) in
            order to
            solve the character recognition problem.</p>
        <p class="lead para">As mentioned in previous section, in Persian OCR it may be better to combine the text line
            segmentation
            process
            and the character recognition process. Therefore, we use a modified version of HMM called Generalized Hidden
            Markov Models (gHMM). The main difference from the classic HMM is that the generalized Viterbi algorithm not
            only iterates over different combinations of characters to find the character sequence with maximum
            probability,
            but it also examines different possible segmentation states in order to find the best segmentation scheme.
            In
            better words, for each segmentation state, the best character sequence is computed using classic HMM, and
            the
            best character sequences of different segmentation states are compared. The details of implementing this
            method
            are explained in Section 8.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="5. Preprocessing on the Scanned Documents"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>5. Preprocessing on the Scanned Documents</h3>
        <p class="lead para">As mentioned in previous section, the images of the text lines of the target document are
            the inputs of the recognition process. However, the very first input of OCR software is the images of target
            document pages. These page images mostly have different types of noises, and sometimes they are skewed.
            These problems can affect the accuracy of the recognition process and should be addressed. Moreover, images
            of the text lines should be detected and extracted from the whole page. In this section, the details of the
            methods used in this project for these types of preprocessing are discussed.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="5.1. Noise Removal"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>5.1. Noise Removal</h3>
        <p class="lead para">Most of the scanned documents, especially older documents and books, include different
            types of noises. The most prevalent type of noises in document images is the extra black or gray pixels in
            white background of the pages, or lighter pixels in the black body of the characters. This type of noises is
            sometimes called salt-and-pepper noises. In the target text used in this project, the presence of
            salt-and-pepper noises is very little. Most of the noises in this text are located around or on the body of
            the characters, as some of them are shown in Figure 5 below.</p>
        <figure class="figure">
            <img src="img/Noise.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="Some examples of noises in the scanned document.">
            <figcaption class="figure-caption text-center">Figure 5 - Some examples of noises in the scanned document
                used in this project
            </figcaption>
        </figure>
        <p>Removing these types of noises is not easy, mostly because removing them needs some intelligence about the
            form
            and structure of characters. In most of OCR software, some types of statistical filters are applied to
            document
            images for noise removal. These filters replace the gray-scale value of each pixel with a weighted average
            of
            the values of the neighbor pixels. Some examples are Gaussian Filter and Median Filter. Unfortunately, our
            tests
            showed that removing noise based on the neighbor pixels is not useful enough for the target text used in
            this
            project. Most of these filters smooth the bodies of characters in practice, and sometimes this leads to the
            removal of the little space between adjacent characters, or the space between the main character body and
            its
            dots and diacritics. As a result of these tests, it was decided not to use any explicit method for noise
            removal.</p>
    </div>
</section>
<!--End section-->

!--Start section-->
<a name="5.2. Skew Detection"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>5.2. Skew Detection</h3>
        <p class="lead para">In order to detect and correct the skew of the pages, some measure is needed so that it can
            be decided whether the page has skew, and if yes, how many degrees. One clue which is widely used for skew
            detection is the fact that in most writings, the deskewed state of the pages has the most number of
            horizontal white lines. Therefore, rotating the input image of the page in different angles and counting the
            number of horizontal pixel lines with a high percentage of white pixels for each angle can be the skew
            detection method.</p>
        <p class="lead para">However, in some writings the mentioned trick may not be enough for skew detection. In this
            project, the product of number of white lines and one other measured parameter is used for this purpose.
            This second parameter is the average of the distance between the horizontal projections of the bounding
            boxes of the detected lines and the horizontal projections of the training line bounding box. This measure
            is computed during the baseline detection stage, and therefore in our implementation the skew detection and
            the baseline detection processes are executed together at once. This second parameter can be understood
            better by reading the next subsection about baseline detection.</p>
        <p class="lead para">In summary, for skew detection firstly the code iterates over different possible skew
            angles, for example angles between -5.0 and 5.0 degrees, with an increment of 0.5 or 0.25. For each angle,
            the page image is rotated by the angle, and then the two mentioned parameters are calculated for the rotated
            page. After analyzing all the possible skew angles, one with the highest product of two parameters is
            selected as the skew angle. The page image rotated by the result angle is used in the next stages of the OCR
            process.</p>
    </div>
</section>
<!--End section-->

!--Start section-->
<a name="5.3. Baseline Detection"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>5.3. Baseline Detection</h3>
        <p class="lead para">Baseline Detection (or Text Line Detection) is the process of detecting the baselines or
            the bounding boxes of the text lines in the image of a document page. The extracted text line images are
            used as input to the next stages of the OCR process. The most common method used for baseline detection is
            using the concept of horizontal projections (or horizontal histograms), which means the number or percentage
            of black pixels in each horizontal pixel line. It is obvious that horizontal projections of the text lines
            in a document have higher values than those of the white space between the text lines, and this observation
            can be useful for baseline detection.</p>
        <p class="lead para">In our approach to baseline detection, firstly we need a typical pattern of the horizontal
            projections of the text lines of the document used in the project. This can be reached by computing the
            average (or more complex statistical measures) of the set of horizontal histograms of some
            manually-extracted text line images. As a result, we have a trained ordered vector of horizontal projections
            that can be compared with different parts of a page image in order to find the text lines. In each page, a
            bounding box (whose width is the same as page width, and its height is the same as the height of typical
            lines of this text) is slid down gradually from top of the page through the bottom. For each part of the
            page, the horizontal projections of the rows are computed and then a distance function is used to compare
            this set with the trained set of horizontal projections. Those parts of the page with the minimum distance
            from the trained set are those which are the best candidates to be the text lines of the page. These
            candidate parts almost always have better distances than their neighbor segments of the page, and thus
            finding the local minima among the distance values leads to detecting the text lines. Figure 6 illustrates
            the effect of executing skew detection and baseline detection programs on part of a page image.</p>
        <figure class="figure">
            <img src="img/Baseline.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="Some examples of noises in the scanned document.">
            <figcaption class="figure-caption text-center">Figure 6 - The main preprocessings performed on one part of
                the image of a page
            </figcaption>
        </figure>
        <p class="lead para">Testing this method on more than 50 pages of the target document, more than 97% of the text
            lines are detected correctly. Some very short text lines (those including a few characters or at
            most one or two words) are not detected correctly. This is because sometimes histogram pattern of very short
            lines does not conform well to the histogram pattern of the training text lines. For example if the phrase
            “عع ” (which is very common in our target text) is the only word in a text line, it is highly probable that
            it is not detected by our baseline detector, because more black pixels are present in the upper part of this
            phrase than in the center rows. Another inefficiency of this method is that in the current implementation,
            the height of the text lines is regarded a fixed value, which is the height of the sliding bounding box. If
            a text includes lines with different or unknown font sizes, the current method may fail in detecting the
            text lines correctly. However, because all the pages of the document used in this project have been written
            with a single font size, we decided not to support multiple font sizes in the current implementation. Using
            a variation of the current method or applying the thresholding method used by [6] for baseline detection can
            solve this problem.</p>
        <p class="lead para">Finally, adequately large white spaces within and around (left and right of) the line
            images are removed, because pure line images with the least quantity of white spaces are needed for the next
            phases. Figure 7 shows a text line before and after the elimination of white spaces:</p>
        <figure class="figure">
            <img src="img/before_after.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="40%" height="40%"
                 alt="Before and after the elimination of extra white spaces.">
            <figcaption class="figure-caption text-center">Figure 7 - A line image before and after the elimination of
                extra white spaces
            </figcaption>
        </figure>
    </div>
</section>
<!--End section-->

!--Start section-->
<a name="6. Features Extraction"></a>
<section class="text-left well well-sm">
    <div class="container">
        <h3>6. Features Extraction</h3>
        <p class="lead para">As mentioned in previous sections, emission probability is computed by measuring the
            similarity between a new character image and a character class. In order to perform this computation, the
            most important features of the character image should be extracted and then these features are compared with
            the important features extracted from the training character images of the character class. Therefore,
            features extraction is an essential stage both in the training phase and in the final text recognition
            process. The set of values of the features extracted from each character image is called the feature vector.
            In this section, some details of
            the features extraction process are explained, as well as the types of features used to represent character
            images in this project.</p>
        <p class="lead para">A good type of feature is one that can help to distinguish better between different
            characters, and to determine better the similarities of images of the same character class. Usually two
            different types of features are used in OCR projects: structural features and statistical features.
            Structural features are those features which are related to the structure and form of the characters and
            their main body and strokes. For example, features about the connected components of a character, or number
            of branches or dots or crosses in the body of the character are among the structural features. On the other
            hand, statistical features are computed by gathering different statistical information from the image of the
            character, rather than the structure or form of the character body itself. Examples are the centroid (i.e.
            center of mass) of black pixels in a character image, or the density of black pixels in different regions of
            the image. Structural features are regarded better in differentiating similar characters, which are numerous
            in Persian alphabet [7] [8]. However, extracting such features correctly in the images is sometimes a
            complex problem. For instance, in the target document of this project, the bodies of characters are mostly
            noisy, and thus structural features such as branches or crosses or loops may not be extracted correctly.
            Moreover, sometimes the dots or inherent diacritics of characters are connected to the main body because of
            noise, and this can prevent successful extraction of features related to dots or other components of
            characters.</p>
        <p class="lead para">Because of these complexities, we decided to use mostly the statistical features in the
            current version of the OCR program. One good semi-structural type of feature is used as well, which is
            called centroid-to-boundary distance [6]. Computing statistical features for smaller regions in the
            character image (in addition to the whole image) is a good practice, because it can better reveal the
            differences in details of the character images. In our project, each character image is resized to a fixed
            size before extracting the features, so that the features values of the character images with different
            sizes are measured based on the same frame and thus can be compared with each other correctly. Also, each
            character image is binarized before features extraction, meaning that the gray-scale values of the pixels
            are converted to either white or black, based on a binarization threshold.</p>
        <p class="lead para">A summary of the main types of features used in this project is given here:</p>
        <ul class="fa-ul">
            <li><i class="fa-li fa fa-circle"></i>
                <p class="lead para">Centroid (or Center of Mass): means the location (x and y) of the
                    center of mass of the black pixels in a
                    character image, or in one of its regions.</p>
            <li><i class="fa-li fa fa-circle"></i>
                <p class="lead para">Number or percentage of black pixels: that is, the percentage of black pixels in
                    different regions of
                    the
                    image.</p>
            </li>
            <li><i class="fa-li fa fa-circle"></i>
                <p class="lead para">Number of rows and columns with black pixels: means number or
                    percentage of rows and columns in the image
                    or its regions that contain at least one black pixel.</p>
            </li>
            <li><i class="fa-li fa fa-circle"></i>
                <p class="lead para">Location of extremum black pixels: is the location (x or y or both of
                    them) of the rightmost, leftmost,
                    uppermost and lowermost black pixel in the character image or one of the smaller regions of the
                    image.
                    Usually the x-coordinate of the rightmost and leftmost black pixels, and the y-coordinate of the
                    uppermost
                    and lowermost black pixels are the most important values in this type of feature.</p>
            </li>
            <li><i class="fa-li fa fa-circle"></i>
                <p class="lead para">Centroid-to-boundary distance: means the distance of the character
                    centroid point from the outer border of
                    the character body, in different angles (e.g. from 0 to 360 degrees, with an increment of 10 or 15
                    degrees).
                    In order to compute this distance for an angle , we should start from the borders of the image in
                    that
                    direction, and then check the pixels toward the centroid point to see where is the first black pixel
                    (which
                    would be the outmost black pixel of the character itself in the direction ). If d is the distance of
                    the
                    centroid from the border of the image in direction , the coordinates of the first pixel to be
                    analyzed
                    are
                    computed as below:</p>
                <figure class="figure">
                    <img src="img/Centroid.png" class="figure-img img-fluid rounded center-block" data-html="true"
                         width="15%" height="15%"
                         alt="Centroid-to-boundary distances shown in different directions.">
                    <figcaption class="figure-caption text-center">Figure 8 - Centroid-to-boundary distances shown in
                        different directions for the character NOON.
                    </figcaption>
                </figure>
            </li>
        </ul>
        <p class="lead para">At first, we intended to use Hough Line and Hough Circle transforms in order to include
            some features about the lines and curves in the body of characters. However, because the implementation used
            was not accurate enough in detecting lines and curves in a suitable way, it was decided to discard these
            types of features. The number of regions used and the parameters
            related to each type of features were determined in cross validation tests (refer to Section 10). The best
            accuracy was achieved by dividing the character image to 6 number of regions (3 horizontal and 2 vertical
            divisions). In this state of parameters, totally 106 number of features form each feature vector.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="7. Training Process"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>7. Training Process</h3>
        <p class="lead para">Training is the process of preparing (or training) the parameters and values needed by the
            recognition process for analyzing and recognizing the text in new document pages. Computing the transition
            and initial probabilities from the language model, and aggregation of the features of training character
            images of various character classes are among the tasks performed in this stage, which are done only once
            during the development of the OCR software before it can be used for recognizing the text in new documents
            and pages.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="7.1. Preparing the Training Character Images"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>7.1. Preparing the Training Character Images</h3>
        <p class="lead para">In order to capture the characteristics of different character classes, the feature vectors
            of the training character images of each class should be aggregated to be used during the recognition
            process for computing the emission probabilities. Collecting the training character images for each
            character class can be a cumbersome task. One way is to manually crop out the character images from the line
            images of the training pages. Each character image should be labeled properly based on the name of its
            corresponding character class, so that later in the training process the program can load and extract
            features from character images of each class.</p>
        <p class="lead para">This manual process can take a long time, and therefore we used another semi-automatic
            method for extracting and labeling the training character images. Firstly, the images of the text lines of
            the pages used for training are extracted and written to disk by the preprocessing part of the program. Then
            in each of the line images, we manually mark the boundary between every two characters by vertical black
            lines with one pixel width. This is done by the means of an image editing software. An example of these
            marked lines can be seen in the Figure 9 below. On the other hand, the text file containing the text of the
            training pages should be prepared (Refer to Section 7.3 about this preparation of the text file). At the end
            of this manual process, a small software program is implemented to extract the character images from the
            marked lines (by detecting the places of vertical black lines), and to save each character image to disk
            with the appropriate label. The character class corresponding to each character image is known by
            reading the type-written text line corresponding to each marked line image. The inclusion of combinatorial
            characters in the list of character classes makes the implementation of this code a little complex. In
            Figure 9, notice some of these combinations of characters which are considered as one single character. At
            the end of this process, the training character images of all the classes are written to disk, and are ready
            to be used as input for the next stage of the training process (as explained in the next section).</p>
        <figure class="figure">
            <img src="img/mark_boundries.png" class="figure-img img-fluid rounded center-block" data-html="true"
                 width="80%" height="80%"
                 alt="Mark boundries between characters">
            <figcaption class="figure-caption text-center">Figure 9 - The boundaries between characters are marked, for
                the later automatic extraction of training characters
            </figcaption>
        </figure>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="7.2. Computing Fundamental Parameters of Emission Probabilities"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>7.2. Computing Fundamental Parameters of Emission Probabilities</h3>
        <p class="lead para">As mentioned earlier, some parameters are needed to hold the most important characteristics
            of each character class, so that they can be used in the recognition process for computing the emission
            probabilities. These representative characteristics are computed by statistically aggregating the feature
            vectors of the training character images. The type of parameters required to compute emission probabilities
            is determined by the function used for computing emission probabilities during the recognition process. As
            it will be explained in Section 8, the function used in this project for this purpose is Modified Quadratic
            Distance Function. This function requires the mean feature vector, the eigenvectors matrix, and the
            eigenvalues matrix of each character class, so that it can compare the feature vector of the new character
            images with all the character classes. Therefore, these three vectors and matrices should be computed for
            each character class in the training process. The mean vector is simply the average of the feature vectors
            of the training characters, and thus its size is the same as the number of features used. The eigenvectors
            and eigenvalues matrices capture more complex statistical information about the features of the training
            character images. If is the number of features, both of these matrices are of size x.</p>
        <p class="lead para">The process of computing these parameters for all character classes is rather
            straightforward. For each class, its training character images are loaded, the features of each image are
            extracted, and the emission parameters for all the training images are computed. These computations are done
            by using the statistical functions of the computer vision library used in this project (Refer to Section 9
            for more information). Finally, the computed parameters for all
            character classes are saved to disk, so that they can be reloaded and used in the recognition process.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="7.3. Computing Language Model Probabilities"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>7.3. Computing Language Model Probabilities</h3>
        <p class="lead para">Language model probabilities include the probabilities which reflect some of the properties
            of the language of the text under analysis. Although the general language properties of the texts written in
            the same language are almost the same, each book or document can have its own specific language properties,
            especially when analyzing older texts in which the types of grammar rules or vocabularies used may be
            different from newer writings in that language. In this project, transition probabilities and initial
            probabilities are among the language model parameters used to improve the accuracy of the final recognition
            process.</p>
    </div>
</section>
<!--End section-->

<!--Start section-->
<a name="7.3.1. Transition Probabilities"></a>
<section class=" text-left well well-sm">
    <div class="container">
        <h3>7.3.1. Transition Probabilities</h3>
        <p class="lead para">Transition probability is a measure of the frequency of the adjacency of two characters in
            the language or the document under analysis. Therefore, the transition probability of two character classes
            and can be computed by the following simple formula:</p>
    </div>
</section>
<!--End section-->

<!--*********************** EDIT ABOVE **********************************-->
<!--Core Scripts-->

<script src="js/core.min.js"></script>
<!--jQuery (necessary for Bootstrap's JavaScript plugins)-->
<!--Include all compiled plugins (below), or include individual files as needed-->
<script src="js/bootstrap.min.js"></script>
<!--Additional Functionality Scripts-->
<script src="js/script.js"></script>

</body>


</html>
